# This is where it all happens...
%Lifecycle::Lite = (
  Name		=> '4Node Lifecycle', # don't worry about this

# These are true global values. Overriding these per dataflow does not make sense
  Quiet		=> 0,
  Verbose	=> 1,
  Debug		=> 0,

  Jitter	=>  0.1, # Spread delay-times for workflow events by a small factor
  CycleSpeedup	=>    1, # speed up time. 1 => real-time, 7 => do a week of work in a day
  StopOnIdle	=>    1, # Not yet implemented...
  Suspend       =>    0, # set to 1 to suspend new workflows from starting,
                         # but allow existing workflows to run to completion
  NJobs         =>    8, # degree of parallelism

##TmpDir	=>  '/tmp/' . (getpwuid($<))[0] . '/', # Directory for temporary files.
# TmpDir should not be shared with other processes, or the garbage-collector may wreak havoc!
  GarbageCycle  =>  300, # How often to run the garbage collector, to clean the TmpDir
##GarbageAge    => 3600, # How old a file can be before being garbage-collected

# Also true globals, but these make sense to override. Providing values here
# is just a convenient way to avoid having to repeat them everywhere.
  CycleTime     => 600,
  NCycles       => -1, # < 0 => infinite, > 0 to limit

  KeepInputs    => 0, # keep the  input files of successful jobs?
  KeepOutputs   => 0, # keep the output files of successful jobs?
  KeepLogs      => 0, # keep the    log files of successful jobs?
  KeepFailedInputs      => 1, # keep the  input files of failed jobs?
  KeepFailedOutputs     => 1, # keep the output files of failed jobs?
  KeepFailedLogs        => 1, # keep the    log files of failed jobs?

# Next, some global values that _can_ be overridden sensibly, per dataflow or per dataset
# StuckFileFraction => 0, # percentage of files stuck in transfer  
# FileSizeMean	 => 2.0, # GB
# FileSizeStdDev => 0.2, # GB

# After the global values, set the Dafaflow default values and dataset workflow
  Templates => {
#   Default parameters for each workflow. These override global defaults, but
#   are in turn overridden by values in the specific workflow instances.
#   Typically, set CycleTime and NFiles, based on the expectations from the
#   computing model in question. For example, NFiles * FileSizeMean / CycleTime
#   (FileSizeMean is set above) gives you the average rate of data 'flowing'
#   through your system. Each of those values can be set per workflow

#   This template will inject data at the T0, subscribe it to the T1s, then...?
    'RAW' => {
      CycleTime		=> 3600 , # Start another instance of this workflow every so often
      Events		=> [ 'makeDataset', 'makeBlocks', 'makeFiles', 'Inject', 'T1Subscribe', 'addData', ],
      Intervals			=> {
	Inject			=>    0,
	T1Subscribe		=>    3,
        addData			=>  600,
	UpdateSubscription	=>    5,
	srcdelete		=>  900,
      },
#     FileSizeStdDev	=>   0.1, # GB - override global default value for this dataflow
      Priority		=> 'low',
      IsCustodial	=>   'n',
      IsMove		=>   'n',
      Group		=> 'operators',
    },
    CheckProxy => {
#     Events       => [ 'CheckProxy' ], # This is the default
      Incarnations =>  1,
    },
    Auth => {
      Incarnations => 1,
      NCycles      => 1,
    },
  },

  Defaults => {
#   Use the Datasvc module to perform the actions, instead of calling the code directly.
    Namespace	=> 'PHEDEX::Testbed::Lifecycle',
    Module => {
      Auth               => 'Datasvc',
      Inject             => 'Datasvc',
      T1Subscribe        => 'Datasvc',
      UpdateSubscription => 'Datasvc',
      srcdelete   => 'Datasvc',
      makeDataset => 'DataProvider',
      makeBlocks  => 'DataProvider',
      makeFiles   => 'DataProvider',
      addData     => 'DataProvider',
    },
    DataProvider => { # parameters for the DataProvider module constructor
      addData  => {
        addEvents => [ 'Inject', 'addData', ], # After running addData, push these events back onto the workflow
      },
    },
    Datasvc	=> { # parameters for the Datasvc module constructor
      url      => 'https://phedex-web-dev.cern.ch/phedex/datasvc',
      instance => 'tony',
#      Set up your proxy by running 'voms-proxy-init --voms cms --valid 192:00'
       cert_file => $ENV{X509_USER_PROXY} || "/tmp/x509up_u$<",
       key_file	 => $ENV{X509_USER_PROXY} || "/tmp/x509up_u$<",
       ca_file	 => $ENV{X509_USER_PROXY} || "/tmp/x509up_u$<",
       ca_dir	 => $ENV{X509_CERT_DIR}   || '/afs/cern.ch/project/gd/LCG-share2/certificates',
    },
    Exec => {
#     External executables for some other actions
      'CheckProxy'	=> 'CheckProxy.pl',
    },
  },

# This is a hash of named workflows. Workflows are based on their 'Template' member,
# and may contain parameters that override the template, or simply add new data that
# is used by separate events in the workflow.
  Workflows => [
    {
      Name			=> 'Raw data',
      Template			=> 'RAW',
      Suspend			=> 0, # Enable/suspend this particular workflow

#     Injection parameters. Only one, so a simple scalar will do
      InjectionSite		=>    'T0_Test_Buffer',

#     addData parameters. How to add data to this dataset
      InjectionsPerBlock	=>         5, # Use open blocks <n> times, then close them
      BlocksPerDataset		=>	   5, # Add <n> blocks to the dataset

#     Subscription parameters. Complex object, and can be multiple.
      T1Subscribe	=> {
        Nodes    => ['T1_Test1_MSS','T1_Test2_MSS','T1_Test3_MSS','T1_Test4_MSS'],
        Priority => 'high',
      },
#     Subscription parameters that are 'global' for this workflow
      IsCustodial	=>        'n',
      IsMove		=>        'n',

#     parameters to exercise UpdateSubscription
      UpdateSubscription => [
	{ node => 'T1_Test1_Buffer',  priority => 'low' },
	{ node => 'T1_Test1_Buffer',  priority => 'normal' },
	{ node => 'T1_Test1_Buffer',  priority => 'high' },
	{ node => 'T1_Test1_Buffer',  group    => 'experts' },
	{ node => 'T1_Test1_Buffer',  group    => 'operators' },
	{ node => 'T1_Test1_Buffer',  suspend_until => 9_999_999_999 },
	{ node => 'T1_Test1_Buffer',  suspend_until => 0, },
      ],

#     Initial parameters for the generator
      Dataset	=> '/tony/test4Node-%02x/RAW',
      Datasets	=>     1,
      Blocks	=>     2,
      Files	=>    15,

      DBS	=> 'http://cmsdoc.cern.ch/cms/aprom/DBS/CGIServer/query',
    },
    { Name => 'CheckProxy', },
    { Name => 'Auth', },
  ],

# These are in case I am using a PhEDEx::Logger to send stuff to. I'm not...
  QueueEntries  => 1,
  RetryInterval => 2,
);

do "$ENV{PHEDEX_CONFIG}/LifecycleNodes.pl";

do "$ENV{PHEDEX_CONFIG}/LifecycleGroups.pl";

# Everything below here can be ignored.
%Logger::Receiver =
(
  ConfigRefresh	=> 10, # Interval for checking config file
  Host		=> 'localhost',
  Port		=> 22201,
# Logfile	=> /tmp/wildish/PhEDEx/logs/prototype.log,
  Quiet		=> 0,
  Verbose	=> 1,
  Debug		=> 0,
);

%Logger::Sender =
(
  QueueEntries	=> 1,
  RetryInterval => 2,
  Quiet		=> 1,
  Verbose	=> 0,
  Debug		=> 0,
);

1;
